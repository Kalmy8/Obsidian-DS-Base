#DataScience 
Одна из простейших моделей машинного обучения, основанная на предположении о том, что целевой признак связан с прочими линейно:
$$\hat{y} = \omega_{0}x_{0}+ \omega_{1}x_{1} + ... + \omega_{n}x_{n},$$
где $\hat{y}$ - предсказанное значение целевого признака,
$\omega_{0,1...n}$ - веса линейной регрессии,
$x_{0,1...n}$ - признаки, на основе которых строится предсказание.

Одним из главных достоинств линейной регрессии является простота её интерпретируемости: после завершения обучения каждый из весов $\omega_{0,1,...n}$ демонстрирует силу и характер связи целевого признака с признаками $x_{0,1,...n}$ соответственно. При этом значения коэффициентов следует трактовать следующим образом:
> Коэффициент (вес) $\omega_{n}$ показывает, как сильно изменится значение целевого признака $y$ при **единичном положительном** изменении предиктора $x_n$