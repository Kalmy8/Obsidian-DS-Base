---
type: note
status: done
tags:
- tech/stack/deepeval
sources:
- null
authors:
- null
---
# DeepEval Core Components

## Component Diagram

```mermaid
graph TB
 %% Data Sources
 Docs[Documents/Contexts] -->|generate| Synth[Synthesizer]
 
 %% Golden Creation
 Synth -->|creates| Golden[Golden<br/>input + expected_output]
 Manual[Manual Creation] -->|creates| Golden
 
 %% Dataset Management
 Golden -->|aggregates| Dataset[EvaluationDataset<br/>Collection of Goldens]
 
 %% Test Case Generation
 Dataset -->|loop through| Golden2[Golden]
 Golden2 -->|invoke LLM app| LLMApp[Your LLM Application]
 LLMApp -->|produces| ActualOutput[actual_output]
 Golden2 -->|combines with| ActualOutput
 ActualOutput -->|creates| TestCase[LLMTestCase<br/>input + actual_output<br/>+ expected_output<br/>+ retrieval_context]
 
 %% Prompt Association
 Prompt[Prompt<br/>template + model params] -->|associated with| TestRun[Test Run<br/>Collection of TestCases<br/>at specific point in time]
 TestCase -->|belongs to| TestRun
 
 %% Evaluation
 TestCase -->|evaluated by| Metrics[BaseMetric<br/>AnswerRelevancyMetric<br/>FaithfulnessMetric<br/>etc.]
 Metrics -->|produces| Scores[Metric Scores]
 Scores -->|attributed to| Prompt
 
 %% Conversational Variants
 ConvGolden[ConversationalGolden<br/>scenario + expected_outcome<br/>+ user_description] -->|aggregates| Dataset
 ConvGolden -->|simulated| ConvTestCase[ConversationalTestCase<br/>scenario + turns<br/>+ expected_outcome]
 ConvTestCase -->|evaluated by| ConvMetrics[Conversational Metrics<br/>TurnRelevancyMetric<br/>etc.]
 
 %% Storage
 Dataset -->|push| ConfidentAI[Confident AI<br/>Cloud Storage]
 Dataset -->|save| LocalFile[Local CSV/JSON]
 TestRun -->|logged to| ConfidentAI
 
 style Golden fill:#e1f5ff
 style Dataset fill:#fff4e1
 style TestCase fill:#e8f5e9
 style Prompt fill:#f3e5f5
 style Metrics fill:#ffebee
 style TestRun fill:#e0f2f1
---
```

## Component Definitions

### **Golden**
- **Purpose**: Template/reference test case
- **Contains**: 
 - `input`: The prompt/query to test
 - `expected_output`: Anticipated response
- **Usage**: Acts as precursor to test cases; doesn't require actual output initially
- **Source**: Can be created manually or generated via `Synthesizer`

### **EvaluationDataset**
- **Purpose**: Collection of `Golden` instances
- **Contains**: 
 - `goldens`: List of `Golden` objects
- **Usage**: Centralized interface for managing and evaluating multiple test cases
- **Storage**: Can be pushed to Confident AI or saved/loaded locally as CSV/JSON

### **LLMTestCase**
- **Purpose**: Individual test case for evaluation
- **Contains**:
 - `input`: The prompt/query
 - `actual_output`: Response from LLM application
 - `expected_output`: Expected response (from Golden)
 - `retrieval_context`: Optional context used during generation
- **Creation**: Generated by invoking LLM app with `Golden.input` and capturing output

### **Prompt**
- **Purpose**: Prompt template and model parameters
- **Contains**:
 - `messages_template`: Structure of prompt (roles + content)
 - Model parameters
- **Usage**: Linked to test runs to attribute metric scores to specific prompts
- **Benefit**: Enables metrics-driven prompt selection and optimization

### **BaseMetric** (and Metrics)
- **Purpose**: Evaluation tools to assess LLM performance
- **Types**: 
 - Generic: `AnswerRelevancyMetric`, `FaithfulnessMetric`, etc.
 - Custom: User-defined metrics
- **Usage**: Applied to `LLMTestCase` to compare actual vs expected outputs
- **Output**: Metric scores attributed to prompts

### **Test Run**
- **Purpose**: Collection of test cases evaluated at a specific point in time
- **Contains**: Multiple `LLMTestCase` instances
- **Usage**: Benchmarks LLM application performance at a snapshot
- **Metadata**: Can include `identifier`, `hyperparameters`, etc.

### **Synthesizer**
- **Purpose**: Generates synthetic `Golden` instances
- **Sources**: Documents, contexts, existing goldens
- **Usage**: Expands evaluation datasets without manual creation

### **ConversationalGolden / ConversationalTestCase**
- **Purpose**: Multi-turn conversation evaluation
- **ConversationalGolden Contains**:
 - `scenario`: Description of conversation scenario
 - `expected_outcome`: Desired result
 - `user_description`: User persona
- **ConversationalTestCase Contains**:
 - `scenario`: Same as Golden
 - `turns`: List of `Turn` objects (role + content)
 - `expected_outcome`: Same as Golden

## Key Interactions

1. **Golden → Dataset**: Multiple `Golden` instances aggregated into `EvaluationDataset`
2. **Dataset → TestCase**: Loop through goldens, invoke LLM app, create `LLMTestCase`
3. **TestCase → Metrics**: Apply metrics to test cases for evaluation
4. **Prompt → TestRun**: Associate prompts with test runs to track performance
5. **Synthesizer → Golden**: Generate goldens from documents automatically

## Evaluation Flow

```
Golden → [Invoke LLM App] → LLMTestCase → [Apply Metrics] → Scores → [Attribute to Prompt]
```

## References
- [DeepEval End-to-End Evals Docs](https://deepeval.com/docs/evaluation-end-to-end-llm-evals)
- [DeepEval Datasets Docs](https://deepeval.com/docs/evaluation-datasets)
- [DeepEval Prompts Docs](https://deepeval.com/docs/evaluation-prompts)
