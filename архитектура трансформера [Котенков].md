[Transformere, explained in detail | Igor Kotenkov | NLP Lecture (in Russian) - YouTube](https://www.youtube.com/watch?v=iOrNbK2T92M)

- Видео по большей части разбирает оригинальную статью "Attention is all you need"
- Начинается всё с истории NLP: 2013 год, появление Word2Vec
	- Порядок обучения модели:
		- **Готовим словарь:** Берем ВСЕ слова, с которыми планируем работать, составляем единый огромнейший массив
			- Слова не из словаря при работе будут просто выброшены
				- Уже видно, насколько метод не гибкий
				- Из-за того возникает необходимость тяжелого препроцессинга:
					- Лемматизация, стемминг
					- Исправление опечаток
					- Обработка спец. символов, чисел
		- **Далее?** CBOW? SKIP-GRAM?
	- Если все сделать правильно - мы получим **список (матрицу) эмбеддингов**. У каждого слова из словаря есть его индекс (положение в списке) и соответствующий вектор длины \<N>. Это и называется "обучить" Word2Vec
		- Длина эмбеддинга идентична для всех слов
	-  Усреднив полученные вектора, мы можем получить что-то вроде **эмбеддинга предложения...**. Правда усреднение - крайне примитивный несовершенный метод, по ходу теряется очень много информации
- **Про токенизацию:**
	- Представим, что мы решили построить Word2Vec, но составляем словарь не из всех слов, а только их отдельных букв
		- Это приведет к тому, что словарь будет состоять всего из 20-100 позиций
		- Но в то же время любое предложение будет содержать тысячи подобных векторов, что очень сильно замедляет время вычислений
			- Я думаю проблема не только в этом?
		- Если же мы будем строить на основе слов - у нас будет огромный словарь (миллионы слов), но зато количество векторов кратно уменьшиться
		- Интуитивно кажется, что хочется как-то найти золотую середину между этими двумя крайностями
		- Такой "серединой" является словарь, **построенный из токенов**:
			- **Токен -** последовательность из нескольких символов (больше, чем один символ, но меньше, чем целое слово)
			- Токены формируются автоматически при помощи алгоритмов типа **BPE (Byte-Pair Encoding) или WordPiece + SentencePiece**
				- **BPE** работает на байтах, универсален для любого языка, используется в GPT моделях
				- Остальные используются в BERT-like моделях и работают для языков, которые можно разбить по пробелу (не-азиатских) 
			- Пример реализации алгоритма:![[Pasted image 20251013222652.png]]
				- На основе всех слов датасета построить **vocabulary - набор всех уникальных символов**
				- **Посчитаем частоты всех пар из vocabulary,** которые реально встречались в наших словах (не все символы со всеми, а только в том порядке, как в наших словах!)
				- Добавляем самую популярную пару в **vocabulary**
				- повторяем итеративно, пока не достигнем **vocabulary_size** - настраиваемого гиперпараметра (при этом получатся и тройки и четверки)
					- Параметр не хотим делать слишком большим, чтобы не включать всякий мусор и слова с опечатками. Вектора на такие токены все равно нормально не обучатся
					- 25-60к - нормальный размер **для одного языка.** Порядка 200к - для нескольких языков (почитать про токензайеры с кластеризацией). В реальности надо попробовать разные варианты и посмотреть, какой именно **vocabulary** получился в результате. По возможности мы не хотим видеть в нем редко встречающиеся специфические наименования/аббревиатуры/словосочетания (потому что эмбеддинги на них все равно не обучатся)
- **Про трансформер:**
	- Исторически возник в результате решения **задачи машинного перевода** при помощи двух моделей: **энкодера и декодера**
		- Энкодер переводил текст  (пр. на французском) в абстрактное векторное пространство (пространство смыслов)
		- Декодер переводил из пространства смыслов на нужный язык
		- Таким образом нет нужды обучать модель под все возможные пары языков
	- ![[Pasted image 20251014225119.png]]
- **Детально про энкодер:**  
- ![[Pasted image 20251021213235.png]]
- Каждый блок имеет одинаковый интерфейс: N эмбеддингов размера D на вход и на выход (это позволяет их стакать)
	- **Self-Attention:** отвечает за коммуникацию. Позволяет модели глядя на токен учитывать контекст (токены вокруг) 
	- **Feed Forward Neural Network:** набор вычислений, которые позволяют "обучать" веса модели (содержит до 75% всех весов трансформера aka "хранилище знаний модели")
	- **Детально про Feed Forward**![[Pasted image 20251021213906.png]]
	- Состоит из двух линейных слоев с функцией типа ReLU
	- Первый слой увеличивает матрицу эмбеддингов в K раз, второй уменьшает обратно
		- K подбирали эмпирически. Если K = 2, то весов в модели будет мало, обучается меньше. Если K = 8, то матрица весов избыточно большая. 4 - часто используемое значение
	- Рассуждаем с позиции матричного умножения (тут лучше нарисовать): 
		- предположим, на вход поступило предложение из 10 токенов типа [1,200,1670...]
		- для каждого токена достали эмбеддинг (вектор размерности M)
		- усреднили их - получили эмбеддинг предложения размерности M
		- подаем вектор D_M на вход блока энкодеров
		- вектор D_M умножается на некоторую **матрицу D_M x D_ff**, получается вектор D_ff, применяется нелинейность типа reLU (отрицательные числа в 0, положительные оставляем)
		- Представим, что в нем все значения в векторе D_ff кроме одного - нули. А это единственное значение - единичка
		- Этот вектор далее будет умножаться на **матрицу D_ff x D_M**
		- ![[Pasted image 20251021220238.png]]
		- **Интуиция: вторая матрица (D_ff x D_M) хранит знания модели, вектор D_ff показывет, с какими весами извлекать информацию из этой матрицы, а первая матрица (D_M x D_ff) позволяет подготовить вектор D_ff на основании эмбеддинга запроса**
		- **Интуиция 2: энкодеры верхнего уровня извлекают сложные знания и смыслы, в то время как энкодеры нижнего - распознают сочетания букв, формируют из них слова**
			- Стакать энкодеры можно хоть до бесконечности, но это сделает вычисления дольше, потому что они работают последовательно! Вместо этого часто имеет смысл увеличивать число весов за счет размерности матриц линейных слоев, а не за счет количества энкодеров
		- **Пример: пользователь ввел запрос "что такое интернет". Попав в энкодер нижнего слоя, **
	- **Детально про Self-Attention:**![[Pasted image 20251021220454.png]]
	- **Интуиция: человеческая речь - сложная вещь. В реальных большинство слов как-то связаны друг с другом**
		- Например : The animal didn't cross the street, because it was tired. Слово "it" очевидно связано со словом "animal" 
		- Тогда давайте обучим **матрицу весов каждого слова с каждым другим.** Мера связи -  0 будет означать отсутствие связи, 0.99 будет означать огромную связь 
	-  ![[Pasted image 20251021221349.png]]
	- Картинка выше описывает процесс на уровне интуиции: каждое слово в предложении мы сопоставляем с остальными, пытаясь найти "связь"
	- ![[Pasted image 20251021221415.png]]
	- Пусть у нас есть предложение из 2х слов: **Thinking Machines**
	- Мы прогоняем его через **Self-Attention, который представлен 3мя матрицами (WQ,WK,WV)**
		- Первые две должны по размерности подходить под длину эмбеддингов токенов, потому что будут перемножаться матрично
		- Про третью позже
	- При умножении на матрицу WQ из эмбеддингов слов мы получаем эмбеддинг **"запрос" (query)**, числа в нем кодируют вопрос от токена к остальной части предложения "что я ищу?"
	- ![[Pasted image 20251021233501.png]]
	- **Затем он умножается скалярно на матрицу WK**. Таким образом как бы сопоставляем "насколько  запрос хорошо подходит к некоторому ключу - шаблону, который я знаю"
		- Важно! Во всем матричном умножении: перемножение матриц как будто бы извлекает "знания" используя "веса". А скалярное произведение как будто бы показывает "меру сходства" между векторами/матрицами (наибольшее число получается, когда вектора сонаправлены, наименьшее - противоположно направлены, 0 - ортогонально направлены)
		- **Затем производится нормировка**-  делим на корень из размерности эмбеддинга
			- Подробнее здесь: [[scaled dot product]]
			- Интуиция: При увеличении дисперсии без нормировки мы будем получать огромные размахи при больших размерностях, при этом softmax будет вырождаться в one-hot вектор [0,0,0,....,1], а градиенты затухать, обучение не идти
		 - **Затем прикладываем softmax,** тогда "меры взаимодействия" каждого слова с каждым в сумме дадут 1 (интуитивно удобно)
	- ![[Pasted image 20251021234820.png]]
	- **Обратим внимание, что вычисление значений softmax - операция квадратичная к длине входной последовательности!**
		- Если входная последовательность содержит в 2  раза больше токенов - делаем в 4 раза больше вычислений
		- ![[Pasted image 20251021235254.png]]
		- С этим можно бороться - например, считать связь только соседних слов (например 15 слов слева и справа от каждого). Это называется sliding window
			- Кстати, поскольку у нас много слоев энкодера --> много слоев self-attention - все-таки даже дальние слова каким-то образом повлияют друг на друга на верхних слоях
	- **Затем** значения softmax умножаются на вектор "values"
		- Вектор values может быть любой размерности, кстати, поэтому матрица WV может иметь любое количество столбцом
		- Матрица как будто бы отвечает на вопрос: "Если ты обратил на меня внимание (твой query похож на мой key)", то какой смысл (value) я хочу до тебя донести? Что я означаю?
		- Пример: Я сказал слово "рама". Оно может означать дверную раму, или раму у картины, или раму автомобиля. Все эти смыслы я могу закодировать в values 
		- У меня вопрос: вообще-то у слова рама уже был эмбеддинг выше (первоначальный вектор). В чем отличие тут? Очевидно, что в лишнем умножении на матрицу, но что это дает?
		- - Интуиция с «рамой»: базовый эмбеддинг слова несёт общий смысл; W_V поворачивает его в подтемы «дверная/картина/авто‑рама». A выбирает, какой из этих сигналов важен в текущем контексте, и смешивает.