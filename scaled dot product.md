---
type: note
status: done
tags: []
sources:
-
authors:
-
---
Вот кратко и по делу.

- 1) Что такое std матрицы

- Обычно берут стандартное отклонение по всем элементам матрицы сразу (или по выбранной оси; в примерах со вниманием чаще “по всем”). Формально: std(X)=1NM∑i,j(Xij−Xˉ)2std(X)=NM1​∑i,j​(Xij​−Xˉ)2​.

- 2) Почему std растёт при умножении/скалярном произведении

- Пусть компоненты независимы с нулевым средним и дисперсиями σq2σq2​ и σk2σk2​.

- Для одного логита внимания s=q⊤k=∑i=1dqikis=q⊤k=∑i=1d​qi​ki​:

- E[qiki]=0E[qi​ki​]=0

- Var(qiki)=E[qi2]E[ki2]=σq2σk2Var(qi​ki​)=E[qi2​]E[ki2​]=σq2​σk2​

- Сумма из d независимых слагаемых: Var(s)=dσq2σk2⇒std(s)=dσqσkVar(s)=dσq2​σk2​⇒std(s)=d​σq​σk​.

- То есть при росте размерности dd разброс логитов растёт как dd​ — именно это вы наблюдаете: значения становятся “крупнее по модулю”, знаки смешаны, и std увеличивается.

- 3) Зачем делить на dkdk​​ (scaled dot‑product)

- Если не масштабировать, логиты softmax имеют std ≈ dkσqσkdk​​σq​σk​. При большом dkdk​ softmax насыщается (почти one‑hot): экспоненты огромные/крошечные, вероятности близки к 0 или 1.

- Градиенты через softmax пропорциональны p(1−p)p(1−p) и тогда стремятся к нулю (vanishing), обучение становится неустойчивым.

- Деление на dkdk​​ нормирует дисперсию логитов к константе (≈ σqσkσq​σk​), держит softmax в “линейной” зоне и стабилизирует градиенты.

- Это согласовано с инициализациями Xavier/He: они выбирают дисперсию весов ∝1/d∝1/d, чтобы сохранять порядок дисперсий при линейных слоях; но в внимании есть скалярное произведение длины dkdk​, поэтому дополнительно масштабируем.

Мини‑проверка в NumPy (покажет рост std как sqrt(d) и коррекцию после деления):

import numpy as np

def check_std(n: int = 5000, d: int = 256) -> tuple[float, float]:

    q = np.random.randn(n, d)  # N(0,1)

    k = np.random.randn(n, d)  # N(0,1)

    logits = (q * k).sum(axis=1)

    return float(logits.std()), float((logits / np.sqrt(d)).std())

raw, scaled = check_std()

print(raw, scaled)  # raw ~ sqrt(d), scaled ~ 1

Интуиция в одном предложении: скалярное произведение суммирует d независимых “микровкладов”, дисперсии суммируются ⇒ std растёт как dd​; деление на dd​ возвращает логиты в стабильный диапазон для softmax и градиентов.